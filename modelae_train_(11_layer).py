# -*- coding: utf-8 -*-
"""modelAE Train (11 Layer)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1erPuUSV7prJSOLQs18CNb2418lMu4GIu

#Training
"""

import os
import numpy as np
import torch
import torchvision
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score

import torch.nn as nn
from torchvision import models

from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import tensorboard

from sklearn.model_selection import train_test_split
import torchvision
from sklearn.manifold import TSNE
from sklearn.cluster import AgglomerativeClustering
import math

from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score
from sklearn.metrics import accuracy_score
from sklearn.manifold import TSNE
from sklearn.metrics import pairwise_distances
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from PIL import Image
import random

from torchvision import models

from sklearn.metrics import accuracy_score

from PIL import Image

from sklearn.model_selection import train_test_split

from itertools import combinations

from tabulate import tabulate

import pandas as pd

"""##Pre processing

"""

from google.colab import drive
drive.mount('/content/drive')

label_mapping = {
    0: 'Pneumonia',
    1: 'Pneumothorax',
    2: 'Tuberculosis',
    3: 'Normal'
}

# Load the pneumonia x-ray dataset into PyTorch
transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize((256, 256)),
    torchvision.transforms.ToTensor()
])

path = '/content/drive/MyDrive/BerkasKuliah/Skripsi/dataset_train'

dataset = torchvision.datasets.ImageFolder(path, transform=transform)
dataloader = DataLoader(dataset, batch_size=1, shuffle=True)

# Perform labeling based on the mapping
labeled_dataset = []
for image, label in dataset:
    labeled_dataset.append((image, label_mapping[label]))

# Display the labeled dataset
for image, label in labeled_dataset:
    print(f"Image: {image}")
    print(f"Label: {label}")
    print()

def count_files(folder_path):
    folder_count = {}
    for root, dirs, files in os.walk(folder_path):
        folder_count[root] = len(files)
    return folder_count

# Provide the folder path you want to count files in
folder_counts = count_files(path)

# Display the file count for each folder
for folder, count in folder_counts.items():
    print(f"Folder: {folder}")
    print(f"Total files: {count}")
    print()

dataset

"""##Autoencoder"""

class EncoderVGG(nn.Module):
    '''Encoder of image based on the architecture of VGG-16 with batch normalization.
    Args:
        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.
            Defaults to True.
    '''
    channels_in = 3
    channels_code = 512

    def __init__(self, pretrained_params=True):
        super(EncoderVGG, self).__init__()

        vgg = models.vgg16_bn(pretrained=pretrained_params)
        del vgg.features[26:]
        del vgg.classifier
        del vgg.avgpool

        self.encoder = self._encodify_(vgg)

    def forward(self, x):
        '''Execute the encoder on the image input
        Args:
            x (Tensor): image tensor
        Returns:
            x_code (Tensor): code tensor
            pool_indices (list): Pool indices tensors in order of the pooling modules
        '''
        pool_indices = []
        x_current = x
        for module_encode in self.encoder:
            output = module_encode(x_current)

            # If the module is pooling, there are two outputs, the second the pool indices
            if isinstance(output, tuple) and len(output) == 2:
                x_current = output[0]
                pool_indices.append(output[1])
            else:
                x_current = output

        return x_current, pool_indices

    @staticmethod
    def dim_code(img_dim):
        '''Convenience function to provide dimension of code given a square image of specified size. The transformation
        is defined by the details of the VGG method. The aim should be to resize the image to produce an integer
        code dimension.
        Args:
            img_dim (int): Height/width dimension of the tentative square image to input to the auto-encoder
        Returns:
            code_dim (float): Height/width dimension of the code
            int_value (bool): If False, the tentative image dimension will not produce an integer dimension for the
                code. If True it will. For actual applications, this value should be True.
        '''
        value = img_dim / 2**5
        int_value = img_dim % 2**5 == 0
        return value, int_value

    def _encodify_(self, encoder):
        '''Create list of modules for encoder based on the architecture in VGG template model.

        In the encoder-decoder architecture, the unpooling operations in the decoder require pooling
        indices from the corresponding pooling operation in the encoder. In VGG template, these indices
        are not returned. Hence the need for this method to extent the pooling operations.
        Args:
            encoder : the template VGG model
        Returns:
            modules : the list of modules that define the encoder corresponding to the VGG model
        '''
        modules = nn.ModuleList()
        for module in encoder.features:
            if isinstance(module, nn.MaxPool2d):
                module_add = nn.MaxPool2d(kernel_size=module.kernel_size,
                                          stride=module.stride,
                                          padding=module.padding,
                                          return_indices=True)
                modules.append(module_add)
            else:
                modules.append(module)

        return modules

class DecoderVGG(nn.Module):
    '''Decoder of code based on the architecture of VGG-16 with batch normalization.

    The decoder is created from a pseudo-inversion of the encoder based on VGG-16 with batch normalization. The
    pesudo-inversion is obtained by (1) replacing max pooling layers in the encoder with max un-pooling layers with
    pooling indices from the mirror image max pooling layer, and by (2) replacing 2D convolutions with transposed
    2D convolutions. The ReLU and batch normalization layers are the same as in the encoder, that is subsequent to
    the convolution layer.
    Args:
        encoder: The encoder instance of `EncoderVGG` that is to be inverted into a decoder
    '''
    channels_in = EncoderVGG.channels_code
    channels_out = 3

    def __init__(self, encoder):
        super(DecoderVGG, self).__init__()

        self.decoder = self._invert_(encoder)

    def forward(self, x, pool_indices):
        '''Execute the decoder on the code tensor input
        Args:
            x (Tensor): code tensor obtained from encoder
            pool_indices (list): Pool indices Pytorch tensors in order the pooling modules in the encoder
        Returns:
            x (Tensor): decoded image tensor
        '''
        x_current = x

        k_pool = 0
        reversed_pool_indices = list(reversed(pool_indices))
        for module_decode in self.decoder:

            # If the module is unpooling, collect the appropriate pooling indices
            if isinstance(module_decode, nn.MaxUnpool2d):
                x_current = module_decode(x_current, indices=reversed_pool_indices[k_pool])
                k_pool += 1
            else:
                x_current = module_decode(x_current)

        return x_current

    def _invert_(self, encoder):
        '''Invert the encoder in order to create the decoder as a (more or less) mirror image of the encoder

        The decoder is comprised of two principal types: the 2D transpose convolution and the 2D unpooling. The 2D transpose
        convolution is followed by batch normalization and activation. Therefore as the module list of the encoder
        is iterated over in reverse, a convolution in encoder is turned into transposed convolution plus normalization
        and activation, and a maxpooling in encoder is turned into unpooling.
        Args:
            encoder (ModuleList): the encoder
        Returns:
            decoder (ModuleList): the decoder obtained by "inversion" of encoder
        '''
        modules_transpose = []
        for module in reversed(encoder):

            if isinstance(module, nn.Conv2d):
                kwargs = {'in_channels' : module.out_channels, 'out_channels' : module.in_channels,
                          'kernel_size' : module.kernel_size, 'stride' : module.stride,
                          'padding' : module.padding}
                module_transpose = nn.ConvTranspose2d(**kwargs)
                module_norm = nn.BatchNorm2d(module.in_channels)
                module_act = nn.ReLU(inplace=True)
                modules_transpose += [module_transpose, module_norm, module_act]

            elif isinstance(module, nn.MaxPool2d):
                kwargs = {'kernel_size' : module.kernel_size, 'stride' : module.stride,
                          'padding' : module.padding}
                module_transpose = nn.MaxUnpool2d(**kwargs)
                modules_transpose += [module_transpose]

        # Discard the final normalization and activation, so final module is convolution with bias
        modules_transpose = modules_transpose[:-2]

        return nn.ModuleList(modules_transpose)

class AutoEncoderVGG(nn.Module):
    '''Auto-Encoder based on the VGG-16 with batch normalization template model. The class is comprised of
    an encoder and a decoder.
    Args:
        pretrained_params (bool, optional): If the network should be populated with pre-trained VGG parameters.
            Defaults to True.
    '''
    channels_in = EncoderVGG.channels_in
    channels_code = EncoderVGG.channels_code
    channels_out = DecoderVGG.channels_out

    def __init__(self, pretrained_params=True):
        super(AutoEncoderVGG, self).__init__()

        self.encoder = EncoderVGG(pretrained_params=pretrained_params)
        self.decoder = DecoderVGG(self.encoder.encoder)

    @staticmethod
    def dim_code(img_dim):
        '''Convenience function to provide dimension of code given a square image of specified size. The transformation
        is defined by the details of the VGG method. The aim should be to resize the image to produce an integer
        code dimension.
        Args:
            img_dim (int): Height/width dimension of the tentative square image to input to the auto-encoder
        Returns:
            code_dim (float): Height/width dimension of the code
            int_value (bool): If False, the tentative image dimension will not produce an integer dimension for the
                code. If True it will. For actual applications, this value should be True.
        '''
        return EncoderVGG.dim_code(img_dim)

    @staticmethod
    def state_dict_mutate(encoder_or_decoder, ae_state_dict):
        '''Mutate an auto-encoder state dictionary into a pure encoder or decoder state dictionary
        The method depends on the naming of the encoder and decoder attribute names as defined in the auto-encoder
        initialization. Currently these names are "encoder" and "decoder".
        The state dictionary that is returned can be loaded into a pure EncoderVGG or DecoderVGG instance.
        Args:
            encoder_or_decoder (str): Specification if mutation should be to an encoder state dictionary or decoder
                state dictionary, where the former is denoted with "encoder" and the latter "decoder"
            ae_state_dict (OrderedDict): The auto-encoder state dictionary to mutate
        Returns:
            state_dict (OrderedDict): The mutated state dictionary that can be loaded into either an EncoderVGG
                or DecoderVGG instance
        Raises:
            RuntimeError : if state dictionary contains keys that cannot be attributed to either encoder or decoder
            ValueError : if specified mutation is neither "encoder" or "decoder"
        '''
        if not (encoder_or_decoder == 'encoder' or encoder_or_decoder == 'decoder'):
            raise ValueError('State dictionary mutation only for "encoder" or "decoder", not {}'.format(encoder_or_decoder))

        keys = list(ae_state_dict)
        for key in keys:
            if 'encoder' in key or 'decoder' in key:
                if encoder_or_decoder in key:
                    key_new = key[len(encoder_or_decoder) + 1:]
                    ae_state_dict[key_new] = ae_state_dict[key]
                    del ae_state_dict[key]

                else:
                    del ae_state_dict[key]

            else:
                raise RuntimeError('State dictionary key {} is neither part of encoder or decoder'.format(key))

        return ae_state_dict

    def forward(self, x):
        '''Forward the autoencoder for image input
        Args:
            x (Tensor): image tensor
        Returns:
            x_prime (Tensor): image tensor following encoding and decoding
        '''
        code, pool_indices = self.encoder(x)
        x_prime = self.decoder(code, pool_indices)

        return x_prime

if torch.cuda.is_available():
 dev = "cuda:0"
else:
 dev = "cpu"
device = torch.device(dev)

modelAE = AutoEncoderVGG().to(device)

"""##Train Loss"""

criterion = torch.nn.MSELoss()

optimizer = torch.optim.Adam(modelAE.parameters(), lr=0.001)

# Set hyperparameters
batch_size = 32
learning_rate = 0.001
num_epochs = 70

# Initialize loss list and accuracy list
loss_values = []
psnr_values = []
accuracy_values = []

# Create SummaryWriter object
writer = SummaryWriter(log_dir='/content/drive/MyDrive/BerkasKuliah/Skripsi/hasiltrain')

# Training loop
total_steps = len(dataloader)
# Initialize scheduler learning rate
scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) #setiap epoch kelipatan 10 lr akan dikaliakn gamma.
for epoch in range(num_epochs):
    correct = 0
    total = 0

    for i, (images, labels) in enumerate(dataloader):
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        outputs = modelAE(images)
        loss = criterion(outputs, images)

        # Calculate PSNR (Peak Signal-to-Noise Ratio)
        mse = torch.mean((images - outputs)**2)
        psnr = 10 * math.log10(1.0 / math.sqrt(mse.item()))

        # Calculate accuracy
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        accuracy = 100 * correct / total

        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Append loss, psnr, and accuracy values to the lists
        loss_values.append(loss.item())
        psnr_values.append(psnr)
        accuracy_values.append(accuracy)

        # Print log info
        if (i+1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], Loss: {loss.item():.4f}, PSNR: {psnr:.2f}, Accuracy: {accuracy:.2f}%')

        # Add loss, psnr, and accuracy values to TensorBoard
        writer.add_scalar('Loss', loss.item(), epoch * total_steps + i)
        writer.add_scalar('PSNR', psnr, epoch * total_steps + i)
        writer.add_scalar('Accuracy', accuracy, epoch * total_steps + i)

# Close SummaryWriter
writer.close()

data = {
    'Loss': loss_values,
    'PSNR': psnr_values,
    'Accuracy' : accuracy_values
}

# Membuat DataFrame dari dictionary
df = pd.DataFrame(data)

# Menyimpan DataFrame ke dalam file Excel
folder_path = '/content/drive/MyDrive/BerkasKuliah/Skripsi/hasiltrain/PelatihanLoss'
# Save the DataFrame to an Excel file
file_path = os.path.join(folder_path, 'Pelatihan_loss_epoch_70(AE).xlsx')
df.to_excel(file_path, index=False)

# Menampilkan grafik garis untuk loss
plt.figure(figsize=(8, 6))
plt.plot(range(len(loss_values)), loss_values)
plt.title('Training Loss')
plt.xlabel('Step')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

# Menampilkan grafik garis untuk PSNR
plt.figure(figsize=(8, 6))
plt.plot(range(len(psnr_values)), psnr_values)
plt.title('PSNR')
plt.xlabel('Step')
plt.ylabel('PSNR')
plt.grid(True)
plt.show()

# Menampilkan grafik garis untuk PSNR
plt.figure(figsize=(8, 6))
plt.plot(range(len(accuracy_values)), accuracy_values)
plt.title('Accuracy')
plt.xlabel('Step')
plt.ylabel('Accuracy')
plt.grid(True)
plt.show()

# Create x-axis values
epochs = range(10, num_epochs+1, 10)

# Create lists to store selected loss and psnr values
selected_loss_values = []
selected_psnr_values = []
selected_accuracy_values = []

# Select loss and psnr values at each epoch
for epoch in epochs:
    selected_loss_values.append(loss_values[(epoch//10)-1])
    selected_psnr_values.append(psnr_values[(epoch//10)-1])
    selected_accuracy_values.append(accuracy_values[(epoch//10)-1])

# Create plots
fig, ax = plt.subplots(3, 1, figsize=(12, 10))
ax = ax.flatten()

# Plot loss
ax[0].plot(epochs, selected_loss_values, label="Loss")
ax[0].set_title("Loss")
ax[0].legend()

# Plot PSNR
ax[1].plot(epochs, selected_psnr_values, label="PSNR")
ax[1].set_title("PSNR")
ax[1].legend()

# Plot Accuracy
ax[2].plot(epochs, selected_accuracy_values, label="Accuracy")
ax[2].set_title("Accuracy")
ax[2].legend()

# Show the plots
plt.show()

"""##Simpan Model Latih"""

import csv

# Specify the file path to save the CSV
csv_file = '/content/drive/MyDrive/BerkasKuliah/Skripsi/hasiltrain/Train_70_AE.csv'

# Open the CSV file in write mode
with open(csv_file, 'w', newline='') as file:
    writer = csv.writer(file)

    # Write the header row
    writer.writerow(['Epoch', 'Loss', 'PSNR', 'Accuracy'])

    # Write the loss values per epoch
    for epoch, loss in enumerate(loss_values, start=1):
        writer.writerow([epoch, loss, psnr, accuracy])

print(f"Loss values saved to {csv_file}")

# Save the trained model
torch.save(modelAE.state_dict(), '/content/drive/MyDrive/BerkasKuliah/Skripsi/model/AE_model_70.pth')

"""##Evaluasi Model Latih"""

model_path = '/content/drive/MyDrive/BerkasKuliah/Skripsi/model/AE_model_70.pth'

if torch.backends.mps.is_available():
    device = torch.device("mps")
    x = torch.ones(1, device=device)
    print(x)
elif torch.cuda.is_available():
    device = torch.device("cuda")
    x = torch.ones(1, device=device)
    print(x)
else:
    print("mps/cuda device not found.")

modelAE.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))
modelAEtrain = modelAE.to(device)
modelAEtrain.train()

encoded_features = []
modelAEtrain.train()
with torch.no_grad():
    for images, _ in tqdm(dataloader, desc='Processing images'):
        images = images.to(device)
        features = modelAE.encoder(images)
        encoded_features.append(features[0].cpu())

encoded_features = torch.cat(encoded_features)
encoded_features = encoded_features.view(encoded_features.size(0), -1)

num_clusters = 15
num_labels = 4

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Perform kmeans clustering on encoded features
# kmeans = KMeans(n_clusters=num_clusters, random_state=42)
# predicted_labels = kmeans.fit_predict(encoded_features)

label_of_interest = 3

# Create a figure and subplots for the grid
fig, axes = plt.subplots(num_labels, 4, figsize=(10, 12))

# Iterate over each label
for label in range(num_labels):
    # Find the indices of images belonging to the current label
    label_indices = [idx for idx, lbl in enumerate(predicted_labels) if lbl == label]

    # Check if there are enough indices for sampling
    if len(label_indices) >= 4:
        # Select 4 random image indices from the current label
        random_indices = random.sample(label_indices, 4)

        # Display the images from the current label in a row
        for i, idx in enumerate(random_indices):
            if idx < len(dataset):
                image, true_label = dataset[idx]
                axes[label, i].imshow(image.permute(1, 2, 0))
                axes[label, i].axis('off')
                axes[label, i].set_title(f"Cluster: {label}, True Label: {true_label}")
    else:
        # Display a message if there are not enough samples for the current label
        axes[label, 0].text(0.5, 0.5, "Not enough samples", ha='center', va='center')
        axes[label, 0].axis('off')

plt.tight_layout()
plt.show()

# Save the figure
fig.savefig('/content/drive/MyDrive/BerkasKuliah/Skripsi/hasiltrain/HasilGambarpelatihan/70_AE_latih_15.png')

# Menginisialisasi matriks untuk menyimpan jumlah label di setiap cluster
cluster_label_counts = [[0] * num_labels for _ in range(num_clusters)]

# Menghitung jumlah label di setiap cluster
for cluster_idx, label_idx in zip(predicted_labels, dataset.targets):
    cluster_label_counts[cluster_idx][label_idx] += 1

# Mencetak hasil clustering citra
for cluster_idx, label_counts in enumerate(cluster_label_counts):
    print(f"Cluster {cluster_idx + 1}:")
    for label_idx, count in enumerate(label_counts):
        print(f"Label {label_idx}: {count}")

# Menginisialisasi matriks untuk menyimpan jumlah label di setiap cluster
cluster_label_counts = [[0] * num_labels for _ in range(num_clusters)]

# Menghitung jumlah label di setiap cluster
for cluster_idx, label_idx in zip(predicted_labels, dataset.targets):
    cluster_label_counts[cluster_idx][label_idx] += 1

# Membuat DataFrame dari matriks cluster_label_counts
df = pd.DataFrame(cluster_label_counts)

# Menambahkan label kolom pada DataFrame
df.columns = [f"Label {label_idx}" for label_idx in range(num_labels)]

# Menambahkan label baris pada DataFrame
df.index = [f"Cluster {cluster_idx + 1}" for cluster_idx in range(num_clusters)]

# Membuat folder jika belum ada
folder_path = '/content/drive/MyDrive/BerkasKuliah/Skripsi/hasiltrain/HasilDatapercluster/ModelAE_Hasil'

# Menyimpan DataFrame ke dalam file Excel
file_path = os.path.join(folder_path, 'hasil_cluster(15)_70_AEtrain.xlsx')
df.to_excel(file_path, index=True)

# Menginisialisasi matriks untuk menyimpan jumlah label di setiap cluster
cluster_label_counts = [[0] * num_labels for _ in range(num_clusters)]

# Menghitung jumlah label di setiap cluster
for cluster_idx, label_idx in zip(predicted_labels, dataset.targets):
    cluster_label_counts[cluster_idx][label_idx] += 1

# Melakukan pelabelan pada setiap cluster
cluster_labels = []
for cluster_idx, label_counts in enumerate(cluster_label_counts):
    # Menentukan label dengan jumlah terbanyak dalam cluster
    max_label_count = max(label_counts)
    max_label_indices = [i for i, count in enumerate(label_counts) if count == max_label_count]
    cluster_label = max_label_indices[0]  # Mengambil label pertama dengan jumlah terbanyak
    cluster_labels.append(cluster_label)

# Membuat pemetaan label kluster dengan label yang diinginkan
label_mapping = {
    0: 'Pneumonia',
    1: 'Pneumothorax',
    2: 'Tuberculosis',
    3: 'Normal'
}

# Mengganti label kluster dengan label yang diinginkan
cluster_labels_mapped = [label_mapping[label] for label in cluster_labels]

# Menampilkan hasil pelabelan
for cluster_idx, label in enumerate(cluster_labels_mapped):
    print(f"Cluster {cluster_idx + 1} label: {label}")


# Menyimpan hasil pelabelan ke dalam file CSV
df = pd.DataFrame({'Cluster': range(1, num_clusters+1), 'Label': cluster_labels_mapped})

df.to_excel('/content/drive/MyDrive/BerkasKuliah/Skripsi/hasiltrain/HasilDatapercluster/ModelAE_Hasil/label_cluster(15)_70_AEtrain.xlsx', index=False)

from sklearn.manifold import TSNE

# Reduce dimensionality of encoded features
tsne = TSNE(n_components=2, random_state=42)
encoded_features_tsne = tsne.fit_transform(encoded_features)

# Plot the clusters
plt.scatter(encoded_features_tsne[:, 0], encoded_features_tsne[:, 1], c=predicted_labels)
plt.title('Clusters Visualization')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.show()